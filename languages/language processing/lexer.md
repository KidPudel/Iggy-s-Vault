The lexer (tokenizer, scanner) is responsible for processing source input into [[token]]s to then fed them to the [[parser]].
This process is called ***lexical analysis***, it extracts the [[token]] and shifts further.

